{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iTransformer: Inverted Transformer for Time Series\n",
    "\n",
    "## üìö Overview\n",
    "\n",
    "**iTransformer** (\"Inverted Transformers Are Effective for Time Series Forecasting\") from ICLR 2024.\n",
    "\n",
    "### Revolutionary Innovation: Inverted Attention\n",
    "\n",
    "**Traditional Transformers:**\n",
    "```\n",
    "[Time1, Time2, Time3, ...] ‚Üí Attention across TIME\n",
    "```\n",
    "\n",
    "**iTransformer:**\n",
    "```\n",
    "[Var1, Var2, Var3, ...] ‚Üí Attention across VARIATES\n",
    "```\n",
    "\n",
    "### Key Advantages\n",
    "- **Variate-centric**: Each variable's full time series becomes a token\n",
    "- **Efficiency**: O(n_variates¬≤) vs O(seq_len¬≤)\n",
    "- **Multivariate**: Excels at capturing cross-variable dependencies\n",
    "- **Better generalization**: More robust to distribution shifts\n",
    "\n",
    "### When to Use iTransformer\n",
    "- ‚úÖ **Multivariate data**: Multiple correlated time series\n",
    "- ‚úÖ **Cross-variable relationships**: Features interact\n",
    "- ‚úÖ **Long sequences**: Efficient on long time series\n",
    "- ‚úÖ **Distribution shift**: Robust to changing patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from models import iTransformerTimeSeriesModel\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Check GPU availability\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Create Multivariate Data\n",
    "\n",
    "iTransformer shines with **multivariate** time series where variables are correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your data or create synthetic multivariate data\n",
    "data_path = '../data/train.csv'\n",
    "\n",
    "if os.path.exists(data_path):\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(f\"Data shape: {df.shape}\")\n",
    "    print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "    \n",
    "    # Extract all numeric columns (multiple variates)\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if len(numeric_cols) > 1:\n",
    "        data = df[numeric_cols].values\n",
    "        print(f\"\\nUsing {len(numeric_cols)} variates: {numeric_cols}\")\n",
    "    else:\n",
    "        # Create additional variates if only 1 column\n",
    "        print(\"\\nOnly 1 numeric column found, creating correlated variates...\")\n",
    "        target = df[numeric_cols[0]].values\n",
    "        var2 = target * 1.2 + np.random.normal(0, 5, len(target))\n",
    "        var3 = target * 0.8 + np.random.normal(0, 3, len(target))\n",
    "        data = np.column_stack([target, var2, var3])\n",
    "        print(f\"Created 3 correlated variates\")\nelse:\n",
    "    print(f\"Data file not found at {data_path}\")\n",
    "    print(\"Creating synthetic multivariate data...\")\n",
    "    \n",
    "    # Create correlated multivariate time series\n",
    "    n_points = 1000\n",
    "    n_variates = 5\n",
    "    \n",
    "    # Base patterns\n",
    "    t = np.arange(n_points)\n",
    "    trend = np.linspace(100, 200, n_points)\n",
    "    seasonality = 20 * np.sin(2 * np.pi * t / 365)\n",
    "    \n",
    "    # Create correlated variates\n",
    "    data = []\n",
    "    for i in range(n_variates):\n",
    "        # Each variate is correlated but with different weights\n",
    "        weight_trend = 0.8 + 0.4 * i / n_variates\n",
    "        weight_season = 1.2 - 0.4 * i / n_variates\n",
    "        noise = np.random.normal(0, 5, n_points)\n",
    "        \n",
    "        variate = weight_trend * trend + weight_season * seasonality + noise\n",
    "        data.append(variate)\n",
    "    \n",
    "    data = np.column_stack(data)\n",
    "    print(f\"Created {n_variates} correlated variates\")\n",
    "\n",
    "print(f\"\\nFinal data shape: {data.shape} (samples, variates)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Visualize Multivariate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_variates = data.shape[1]\n",
    "\n",
    "fig, axes = plt.subplots(n_variates, 1, figsize=(15, 3*n_variates))\n",
    "if n_variates == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i in range(n_variates):\n",
    "    axes[i].plot(data[:, i], linewidth=1.5)\n",
    "    axes[i].set_title(f'Variate {i+1}', fontsize=12, fontweight='bold')\n",
    "    axes[i].set_ylabel('Value')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "axes[-1].set_xlabel('Time Step')\n",
    "plt.suptitle('Multivariate Time Series (All Variates)', fontsize=14, fontweight='bold', y=1.0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show correlation between variates\n",
    "plt.figure(figsize=(10, 8))\n",
    "import seaborn as sns\n",
    "correlation_matrix = np.corrcoef(data.T)\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "            xticklabels=[f'Var{i+1}' for i in range(n_variates)],\n",
    "            yticklabels=[f'Var{i+1}' for i in range(n_variates)])\n",
    "plt.title('Correlation Between Variates', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Correlation Analysis:\")\n",
    "print(\"High correlation means variates influence each other\")\n",
    "print(\"‚Üí iTransformer can learn these cross-variate relationships!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Prepare Data for iTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/validation split\n",
    "split_idx = int(len(data) * 0.8)\n",
    "train_data = data[:split_idx]\n",
    "val_data = data[split_idx:]\n",
    "\n",
    "print(f\"Train data shape: {train_data.shape}\")\n",
    "print(f\"Validation data shape: {val_data.shape}\")\n",
    "print(f\"\\nNumber of variates: {data.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Initialize iTransformer Model\n",
    "\n",
    "### Key Difference from Standard Transformer\n",
    "\n",
    "**Standard Transformer:**\n",
    "- Input shape: [batch, seq_len, features]\n",
    "- Attention: Across time steps\n",
    "- Each time step is a token\n",
    "\n",
    "**iTransformer:**\n",
    "- Input shape: [batch, features, seq_len] ‚Üê **Inverted!**\n",
    "- Attention: Across variates/features\n",
    "- Each variate's time series is a token\n",
    "\n",
    "### Hyperparameters\n",
    "- **seq_len**: Input window\n",
    "- **pred_len**: Forecast horizon  \n",
    "- **n_features**: Number of variates (variables)\n",
    "- **d_model**: Embedding dimension (larger = more capacity)\n",
    "- **n_heads**: Attention heads\n",
    "- **n_layers**: Transformer layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "seq_len = 96       # Input window\n",
    "pred_len = 24      # Forecast horizon\n",
    "n_features = data.shape[1]\n",
    "\n",
    "# Initialize iTransformer\n",
    "model = iTransformerTimeSeriesModel(\n",
    "    seq_len=seq_len,\n",
    "    pred_len=pred_len,\n",
    "    n_features=n_features,  # Each variate becomes a token!\n",
    "    d_model=512,            # Embedding dimension (can be large since n_features usually small)\n",
    "    n_heads=8,\n",
    "    n_layers=2,             # 2-3 layers usually sufficient\n",
    "    d_ff=2048,\n",
    "    dropout=0.1,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    learning_rate=0.001,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"\\niTransformer Model Initialized\")\n",
    "print(f\"Input: {seq_len} time steps √ó {n_features} variates\")\n",
    "print(f\"Output: {pred_len} time steps √ó {n_features} variates\")\n",
    "print(f\"\\nAttention mechanism: Across {n_features} variates (not time!)\")\n",
    "print(f\"Complexity: O({n_features}¬≤) instead of O({seq_len}¬≤)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train iTransformer\n",
    "print(\"Training iTransformer...\")\n",
    "print(\"This learns cross-variate dependencies!\\n\")\n",
    "\n",
    "metrics = model.train(train_data, val_data, verbose=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Training Complete!\")\n",
    "print(\"=\"*50)\n",
    "if 'val_rmse' in metrics:\n",
    "    print(f\"Validation RMSE: {metrics['val_rmse']:.4f}\")\n",
    "if 'val_mae' in metrics:\n",
    "    print(f\"Validation MAE: {metrics['val_mae']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Visualize Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'train_losses' in metrics and 'val_losses' in metrics:\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.plot(metrics['train_losses'], label='Training Loss', linewidth=2)\n",
    "    plt.plot(metrics['val_losses'], label='Validation Loss', linewidth=2)\n",
    "    \n",
    "    best_epoch = np.argmin(metrics['val_losses'])\n",
    "    plt.axvline(best_epoch, color='r', linestyle='--', alpha=0.5, \n",
    "                label=f'Best Epoch: {best_epoch}')\n",
    "    \n",
    "    plt.xlabel('Epoch', fontsize=12)\n",
    "    plt.ylabel('Loss (MSE)', fontsize=12)\n",
    "    plt.title('iTransformer Training History', fontsize=14, fontweight='bold')\n",
    "    plt.legend(fontsize=11)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÆ Make Multivariate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions = model.predict(val_data, return_sequences=False)\n",
    "\n",
    "print(f\"Predictions shape: {predictions.shape}\")\n",
    "print(f\"(samples, features) = ({predictions.shape[0]}, {predictions.shape[1]})\")\n",
    "print(f\"\\nFirst prediction (all variates): {predictions[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Visualize Multivariate Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a specific window\n",
    "test_idx = 0\n",
    "context = val_data[test_idx:test_idx + seq_len]\n",
    "\n",
    "# Make forecast for all variates\n",
    "forecast = model.predict(context.reshape(1, seq_len, n_features))\n",
    "if forecast.ndim == 3:\n",
    "    forecast = forecast[0]  # Remove batch dimension\n",
    "\n",
    "# Get actual future values\n",
    "actual_future = val_data[test_idx + seq_len:test_idx + seq_len + pred_len]\n",
    "if len(actual_future) < pred_len:\n",
    "    forecast = forecast[:len(actual_future)]\n",
    "\n",
    "# Plot each variate\n",
    "fig, axes = plt.subplots(n_features, 1, figsize=(15, 4*n_features))\n",
    "if n_features == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i in range(n_features):\n",
    "    # Historical context\n",
    "    axes[i].plot(range(seq_len), context[:, i], \n",
    "                 label='Historical', linewidth=2, alpha=0.5, color='blue')\n",
    "    \n",
    "    # Forecast vs actual\n",
    "    forecast_range = range(seq_len, seq_len + len(actual_future))\n",
    "    axes[i].plot(forecast_range, actual_future[:, i], 'g-',\n",
    "                 label='Actual', linewidth=2.5, marker='o', markersize=5)\n",
    "    axes[i].plot(forecast_range, forecast[:, i], 'r--',\n",
    "                 label='Forecast', linewidth=2.5, marker='s', markersize=4)\n",
    "    \n",
    "    axes[i].axvline(seq_len - 1, color='black', linestyle='--', alpha=0.3)\n",
    "    axes[i].set_title(f'Variate {i+1} - Forecast', fontsize=12, fontweight='bold')\n",
    "    axes[i].set_ylabel('Value')\n",
    "    axes[i].legend(fontsize=10)\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "axes[-1].set_xlabel('Time Step')\n",
    "plt.suptitle('iTransformer: Multivariate Forecasts', fontsize=14, fontweight='bold', y=1.0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate error per variate\n",
    "print(\"\\nüìä Per-Variate Forecast Accuracy:\")\n",
    "for i in range(n_features):\n",
    "    mae = np.mean(np.abs(actual_future[:, i] - forecast[:, i]))\n",
    "    print(f\"Variate {i+1} MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî¨ Analyze Cross-Variate Attention\n",
    "\n",
    "One of iTransformer's superpowers: learning which variates influence each other!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions for multiple samples\n",
    "n_samples = min(100, len(val_data) - seq_len - pred_len)\n",
    "errors_per_variate = []\n",
    "\n",
    "for i in range(n_samples):\n",
    "    context = val_data[i:i + seq_len]\n",
    "    forecast = model.predict(context.reshape(1, seq_len, n_features))\n",
    "    if forecast.ndim == 3:\n",
    "        forecast = forecast[0]\n",
    "    \n",
    "    actual = val_data[i + seq_len:i + seq_len + pred_len]\n",
    "    if len(actual) == pred_len:\n",
    "        errors = np.abs(actual - forecast)\n",
    "        errors_per_variate.append(errors.mean(axis=0))\n",
    "\n",
    "errors_per_variate = np.array(errors_per_variate)\n",
    "\n",
    "# Visualize average error per variate\n",
    "plt.figure(figsize=(12, 6))\n",
    "mean_errors = errors_per_variate.mean(axis=0)\n",
    "std_errors = errors_per_variate.std(axis=0)\n",
    "\n",
    "x = np.arange(n_features)\n",
    "plt.bar(x, mean_errors, yerr=std_errors, capsize=5, alpha=0.7, color='steelblue')\n",
    "plt.xlabel('Variate', fontsize=12)\n",
    "plt.ylabel('Mean Absolute Error', fontsize=12)\n",
    "plt.title('iTransformer Performance per Variate', fontsize=14, fontweight='bold')\n",
    "plt.xticks(x, [f'Var {i+1}' for i in range(n_features)])\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Interpretation:\")\n",
    "print(\"Lower error = better prediction for that variate\")\n",
    "print(\"‚Üí Some variates may be easier to predict due to stronger cross-variate relationships\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model_path = '../trained_models/itransformer_model.pth'\n",
    "os.makedirs('../trained_models', exist_ok=True)\n",
    "\n",
    "model.save_model(model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Key Takeaways\n",
    "\n",
    "### iTransformer Innovation\n",
    "1. **Inverted Attention**: Attends across variates, not time\n",
    "2. **Variate-Centric**: Each variable's time series is a token\n",
    "3. **Efficient**: O(n_variates¬≤) complexity\n",
    "4. **Cross-Dependencies**: Learns how variables influence each other\n",
    "\n",
    "### iTransformer vs PatchTST vs Standard Transformer\n",
    "\n",
    "| Model | Attention | Best For | Complexity |\n",
    "|-------|-----------|----------|------------|\n",
    "| Standard Transformer | Time steps | Univariate | O(seq_len¬≤) |\n",
    "| PatchTST | Patches | Long sequences | O(n_patches¬≤) |\n",
    "| iTransformer | Variates | Multivariate | O(n_variates¬≤) |\n",
    "\n",
    "### When to Use iTransformer\n",
    "- ‚úÖ **Multivariate data**: 2+ correlated time series\n",
    "- ‚úÖ **Cross-variable relationships**: Variables influence each other\n",
    "- ‚úÖ **Many variates**: Efficient even with 50+ variables\n",
    "- ‚úÖ **Distribution shift**: More robust than standard transformers\n",
    "- ‚úÖ **Feature-rich**: When you have many related measurements\n",
    "\n",
    "### When NOT to Use iTransformer\n",
    "- ‚ùå **Univariate data**: Use PatchTST or TimesNet instead\n",
    "- ‚ùå **Independent variates**: If variables don't correlate\n",
    "- ‚ùå **Very few variates** (1-2): Overhead not worth it\n",
    "\n",
    "### Hyperparameter Tips\n",
    "1. **d_model**: Can be large (512-1024) since n_features usually small\n",
    "2. **n_layers**: 2-3 layers sufficient (more may overfit)\n",
    "3. **n_heads**: 8 heads works well\n",
    "4. **Learning rate**: 0.0001-0.001\n",
    "\n",
    "### Real-World Applications\n",
    "- **Finance**: Multiple stock prices, economic indicators\n",
    "- **Energy**: Multiple sensor readings from power grid\n",
    "- **Weather**: Temperature, humidity, pressure across locations\n",
    "- **Healthcare**: Multiple vital signs (heart rate, BP, temp)\n",
    "- **IoT**: Multiple sensor measurements\n",
    "\n",
    "### Next Steps\n",
    "1. Try with your multivariate data\n",
    "2. Compare with univariate models (PatchTST)\n",
    "3. Analyze which variates are most predictable\n",
    "4. Combine with other models in ensemble\n",
    "5. Experiment with different numbers of variates"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
